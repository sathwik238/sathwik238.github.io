---
title: "Analyzing Caesarian Dataset"
author: "Sathwik Bollepalli"
date: "11/05/2023"
format:
  html:
    embed-resources: true
    theme: cosmo
    code-line-numbers: true
    number_examples: true
    number_sections: true
    number_chapters: true
    linkcolor: blue
editor: visual
fig-cap-location: top
---

**Problem 1:**

```{r}
caesarian=read.csv('data/caesarian.csv')
head(caesarian)
```

Data Processing

Converting categories into factors

```{r}
caesarian$delivery_number <- as.factor(caesarian$delivery_number)
caesarian$delivery_time <- as.factor(caesarian$delivery_time)
caesarian$blood_pressure <- as.factor(caesarian$blood_pressure)
caesarian$heart_problem <- as.factor(caesarian$heart_problem)
caesarian$Caesarian <- as.factor(caesarian$Caesarian)
```

```{r}
set.seed(123457)
train.prop <- 0.80
strats <- caesarian$Caesarian
rr <- split(1:length(strats), strats)
idx <- sort(as.numeric(unlist(sapply(rr, 
        function(x) sample(x, length(x)*train.prop)))))
caesarian.train <- caesarian[idx, ]
caesarian.test <- caesarian[-idx, ]
```

Lets check the split of the dependant variable in the train test and entire dataset.

```{r}
summary(caesarian.train$Caesarian)/nrow(caesarian.train)
```

```{r}
summary(caesarian.test$Caesarian)/nrow(caesarian.test)
```

```{r}
summary(caesarian$Caesarian)/nrow(caesarian)
```

**a.**

```{r}
model_age.logit <- glm(Caesarian ~ age ,data = caesarian.train, 
                  family = binomial(link = "logit"))
summary(model_age.logit)
```

The estimated intercept is -0.67091. This is the log-odds of having a Caesarian when age is 0.

For a one-unit increase in age, the log-odds of having a Caesarian increase by 0.03445.

As we can see both the null deviance and the residual deviance are very close, we can infer that intercept is alone enough and age alone is not enough to predict the dependent variable.

**b.**

```{r}
final_model <- step(
  model_age.logit,
  scope = list(
    lower = ~ age,
    upper =  ~ age + delivery_number + delivery_time + blood_pressure + heart_problem
  ),
  direction = "forward"
)
#formula(final_model)
summary(final_model)
```

Among the other predictors except Age, if we see the deviances heart_problems with age gives us the least residual of 75.413 and least AIC with 81.41.

The deviance has reduced by 10 units from 85.5 to 75.40.

The model with age and heart_problrms converges in 4 iterations.

The intercept for heartproblrms whcih is 1.81 indicates the increase in log-odds for presence of heartdisease1 considering other variables constant.

This variable is significant, shown by the low p-value of 0.00301.

**c.**

```{r}
full.logit <- glm(Caesarian ~ . ,data = caesarian.train, 
                  family = binomial(link = "logit"))
summary(full.logit)
```

Intercept 2.1 means that the log-odds when all the variables are 0.

For each additional year of age, the log-odds of having a Caesarian section decreases by about 0.0255, provided other variables are constant.

Blood pressure1 seems to be significant, meaning people having bloodpressure1 as factor decrease their log-odds by 2.54.

The residual deviance is far better than the null devaince, which mean the full model is better compared to null model.

$$
H_0:\text{Data prefers Full model}\\
H_a:\text{Data does not prefer the Full model}
$$

```{r}
with(full.logit, cbind(deviance = deviance, df = df.residual,
     p = pchisq(deviance, df.residual, lower.tail=FALSE)))
```

The Chi-Squared test suggests that the full model is preferred as the p-value is grater than 0.05.

**d.**

```{r}
fitted_values<-ifelse(fitted(full.logit)>0.65,1,0)
confusion_matrix <- table(fitted_values, caesarian.train$Caesarian)
print(confusion_matrix)
```

The cell (0,0) with value 24 says that the model correctly predicted 24 instances where no Caesarian section was performed.

The cell (0,1) with value 12 says that the model incorrectly predicted 12 instances of NO Caesarian but actually they were performed.

The cell (1,0) with value 3 says that the model incorrectly predicted 3 instances where Caesarian was performed but actually they were not performed.

The cell (1,1) with value 24 says that the model correctly predicted 24 instances where a Caesarian section was performed.

```{r}
TP <- confusion_matrix[2, 2]
TN <- confusion_matrix[1, 1]
FP <- confusion_matrix[2, 1]
FN <- confusion_matrix[1, 2]

Total <- sum(confusion_matrix)

accuracy <- (TP + TN) / Total

sensitivity <- TP / (TP + FN)

specificity <- TN / (TN + FP)


cat("Accuracy:", accuracy, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")
```

The model give an accuracy of (24+24)/(24+12+3+24) = 0.76. Which is 76%.

It gives us a sensitivity of 66%, means the model predicted 66% of Caesarians(Actual yes) as Caesarians(yes predicted).

It gives us a Specificity of 88%, means the model predicted 88% of non-Caesarians(Actual NO) as Non-Caesarians(No predicted).

While the model shows a decent fit with good accuracy and high specificity, its moderate sensitivity suggests that there might be a need for further tuning

Evaluating on testing data

```{r}
predicted_values<-ifelse(predict(full.logit,newdata = caesarian.test, type = 'response')>0.65,1,0)
confusion_matrix_test <- table(predicted_values, caesarian.test$Caesarian)
print(confusion_matrix_test)
```

```{r}
TP <- confusion_matrix_test[2, 2]
TN <- confusion_matrix_test[1, 1]
FP <- confusion_matrix_test[2, 1]
FN <- confusion_matrix_test[1, 2]

Total <- sum(confusion_matrix_test)

accuracy <- (TP + TN) / Total

sensitivity <- TP / (TP + FN)

specificity <- TN / (TN + FP)


cat("Accuracy:", accuracy, "\n")
cat("Sensitivity:", sensitivity, "\n")
cat("Specificity:", specificity, "\n")

```

On the test Data set:

The model gives an accuracy of 59%, whcih means the model predictions are 59% accurate, it predicted 59 times correct out of 100 samples.

The Sensitivity is 50% of the true positive cases were correctly identified.

The specificity is 71% of the true negative cases were correctly identified.

**Problem 2:**

```{r}
leveefailure=read.csv("data/leveefailure.csv")
leveefailure <- subset(leveefailure, select = -X)

head(leveefailure)
```

Data Pre-Processing

Converting to factor variables

```{r}
leveefailure$Landcover <- as.factor(leveefailure$Landcover)
leveefailure$Meander <- as.factor(leveefailure$Meander)
leveefailure$Borrowpit <- as.factor(leveefailure$Borrowpit)
leveefailure$Sediments <- as.factor(leveefailure$Sediments)
leveefailure$Failure <- as.factor(leveefailure$Failure)
leveefailure$Revetment <- as.factor(leveefailure$Revetment)
```

Train Test Split

```{r}
set.seed(123457)
train.prop <- 0.80
strats <- leveefailure$Failure
rr <- split(1:length(strats), strats)
idx <- sort(as.numeric(unlist(sapply(rr, 
        function(x) sample(x, length(x)*train.prop)))))
leveefailure.train <- leveefailure[idx, ]
leveefailure.test <- leveefailure[-idx, ]
```

```{r}
table(leveefailure$Failure)
```

The data set is equally ablanced between True and false values for the Failure Independent variables.

```{r}
summary(leveefailure.train$Failure)/nrow(leveefailure.train)
```

```{r}
summary(leveefailure$Failure)/nrow(leveefailure)
```

```{r}
summary(leveefailure.test$Failure)/nrow(leveefailure.test)
```

We can see that the proportions of the two levels of the response-FailureÂ are the same in the train, test, and the entire data sets.

```{r}
levee.full.logit <- glm(Failure ~ .  ,data = leveefailure.train, 
                  family = binomial(link = "logit"))
summary(levee.full.logit)
```

The intercept value represents the change in log-oddds of failure when all the predictors are 0 which is -2.24e02.

The year has a coefficient of 0.133 and p-value of 0.03, suggesting that the year is a statistically significant predictor of failure, with each passing year slightly increasing the log odds of failure.

Dredging with a coefficient of 5.93e-06 appears to be a significant predictor at the 0.05 level, slightly decreasing the log odds of failure.

Rivermile, Sediments1, Borrowpit1, etc.: These predictors have higher p-values, suggesting that they are not statistically significant at the 0.05 level.

The null deviance is high compared to the residual deviance, which suggests that the model has some predictive power.

The AIC is used to compare models, with lower values generally indicating a better model fit. This model has an AIC of 81.187.

The model took 17 iterations to converge, which is relatively high and might suggest some issues with model convergence.

```{r}
levee.null.logit <- glm(Failure~1, data = leveefailure.train, 
                  family = binomial(link = "logit"))
summary(levee.null.logit)
```

The null model with a no varaibles fives us deviance of 88.7 with AIC of 90.7.

$$
H_0:\text{Data prefers Full model}\\
H_a:\text{Data does not prefer the Full model}
$$

```{r}
with(levee.full.logit, cbind(deviance = deviance, df = df.residual,
     p = pchisq(deviance, df.residual, lower.tail=FALSE)))
```

The high p-value of 0.5 suggests that the model prefers the full model than the null model.

The following steps show how we can use both backward and forward selection to choose predictors for explaining the response Failure

```{r}
both.logit <-
  step(
    levee.null.logit,
    list(
      lower = formula(levee.null.logit),
      upper = formula(levee.full.logit)
    ),
    direction = "both",
    trace = 0,
    data = leveefailure.train
  )
formula(both.logit)
```

The step function iteratively adds and removes predictors from the model to find a combination that gives a good balance between model complexity and fit.

The predictors that were not included in the final model were found not to improve the model sufficiently to justify their inclusion based on the AIC criterion.

```{r}
levee.both.logit = glm(Failure~Sinuosity + ChannelWidth + Floodwaywidth + Meander , data=leveefailure.train,family = binomial(link = "logit"))
summary(levee.both.logit)
```

A one-unit increase in Sinousity is associated with an increase in the log odds of a failure by approximately 1.768 units, holding all other variables constant.

The reference category (Meander1) is omitted, so these coefficients should be interpreted relative to it. For example, being at Meander2 (outside bend) is associated with a decrease in the log odds of failure by approximately 1.762 units compared to being at an inside bend (Meander1).

To conclude, the model does not have any predictors that are statistically significant at the 0.05 level, but Sinuosity, ChannelWidth, and Floodwaywidth show potential as they have p-values less than 0.1.

```{r}
levee.null.logit$deviance
levee.full.logit$deviance
levee.both.logit$deviance
```

As we ca see the full model has the least residual deviance on the train data.

Therefore, the full model is preferred on the training data set.

#### Assess test data accuracy

```{r}
pred.both <- predict(levee.both.logit, newdata = leveefailure.test, type="response")
pred.full <- predict(levee.full.logit, newdata = leveefailure.test, type="response")
```

```{r}
(table.both <- table(pred.both > 0.5, leveefailure.test$Failure))
```

```{r}
(table.full <- table(pred.full > 0.5, leveefailure.test$Failure))
```

```{r}
(accuracy.both <- round((sum(diag(table.both))/sum(table.both))*100,2)) 
```

```{r}
(accuracy.full <- round((sum(diag(table.full))/sum(table.full))*100,2))
```

Out of the both anf full model, the levee.both.logit model performs a bit better than the full model with accuracy of 54%, Yes, this accuracy is low, this is because of the less number of data point in out test data set which is due to the lack of observation in our original dataset.

Another useful metric is area under theÂ **receiver operating characteristics**Â (ROC) curve. The ROC curve is a metric used to evaluate the prediction accuracy in binary and multi-class classification.

```{r}
library(pROC)
roc.both <- roc(leveefailure.test$Failure, pred.both, levels=c(1,0))
roc.both
```

```{r}
roc.full <- roc(leveefailure.test$Failure, pred.full, levels=c(1,0))
roc.full
```

The ROC is a probability curve, and AUC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes.

We can see that the AUC-ROC is higher for the full model than the both.logit model.

Let's try the **Backward elimination for variable selection:**

```{r}
backwards <- step(levee.full.logit, trace = 0)  #suppress details of each iteration
formula(backwards)
```

```{r}
summary(backwards)
```

For Year, an increase of one year is associated with an increase in the log odds of failure by 0.1112.

For RiverMile, an increase of one mile is associated with a decrease in the log odds of failure by 0.01892.

The model suggests that Year, Rivermile, Meander2, Sinuosity, and Dredging are significant predictors of levee failure.

The positive coefficients for Year and Sinousity indicate that more recent years and greater sinuosity are associated with higher odds of failure, while the negative coefficient for Rivermile and Dredging suggest that locations further along the river and higher dredging activities are associated with lower odds of failure.

The AIC here is 76.644.

```{r}
pred.back <- predict(backwards, newdata = leveefailure.test, type="response")
```

```{r}
(table.back <- table(pred.back > 0.5, leveefailure.test$Failure))
```

```{r}
(accuracy.back <- round((sum(diag(table.both))/sum(table.back))*100,2)) 
```

```{r}
roc.back <- roc(leveefailure.test$Failure, pred.back, levels=c(1,0))
roc.back
```

As we can see the model from the backwards elimination with the formula Failure \~ Year + Rivermile + Meander + Floodwaywidth + ConstrictionFactor + Landcover + Sinuosity + Dredging, is having the lowest AIC of 76.6 compared to both.logit and full.logit models.

It also has a low deviance of 50.6.

It also has a better accuracy than the full model of 55% but similar to the both.logit model.

It also has an high AUC-ROC of 0.67 compared to the other models, a high AUC-ROC means the model has a good measure of separability and is making predictions with high confidence.
