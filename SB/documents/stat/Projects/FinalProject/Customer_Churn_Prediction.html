<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Adam Syed and Sathwik Bollepalli">

<title>Customer Churn Prediction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="Customer_Churn_Prediction_files/libs/clipboard/clipboard.min.js"></script>
<script src="Customer_Churn_Prediction_files/libs/quarto-html/quarto.js"></script>
<script src="Customer_Churn_Prediction_files/libs/quarto-html/popper.min.js"></script>
<script src="Customer_Churn_Prediction_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Customer_Churn_Prediction_files/libs/quarto-html/anchor.min.js"></script>
<link href="Customer_Churn_Prediction_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Customer_Churn_Prediction_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Customer_Churn_Prediction_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Customer_Churn_Prediction_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Customer_Churn_Prediction_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Customer Churn Prediction</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Adam Syed and Sathwik Bollepalli </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract"><strong>Abstract</strong></h2>
<p>In the competitive banking industry, retaining customers is crucial for success. This project, “<strong>Customer Churn Prediction</strong>,” aims to predict and address customer churn effectively using data science and machine learning.</p>
<p>We use the Bank Customer Churn Dataset from Kaggle, preprocess it to handle missing data. Exploratory Data Analysis helps us understand data patterns, and we build predictive models like logistic regression, decision trees, and random forests. Model selection is based on metrics like accuracy, precision, recall, and F1-score.</p>
<p>Expected outcomes include a predictive model for identifying high-risk customers, insights into churn factors, and retention strategy recommendations. These insights will improve customer satisfaction, loyalty, cost efficiency, and competitive advantage for ABC Bank.</p>
<p>In conclusion, this project leverages data science to transform ABC Bank’s customer retention strategies and foster a data-driven culture for a more loyal customer base.</p>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction"><strong>Introduction</strong>:</h2>
<p>In today’s fast-paced banking sector, customer retention emerges as a critical factor for the sustained success and growth of financial institutions. ABC Bank, recognizing the importance of maintaining a loyal customer base, has embarked on a strategic project focusing on the prediction of customer churn. This initiative is driven by the need to understand and mitigate the factors leading to customer attrition, a phenomenon that not only affects revenue but also impacts the bank’s long-term market positioning and reputation.</p>
<p>The core objective of this project is to develop a robust predictive model using advanced data science techniques, which will enable ABC Bank to identify at-risk customers proactively. By leveraging historical data, including transaction patterns, account activities, and customer demographics, the model aims to uncover hidden patterns and indicators of potential churn. The insights gained from this analysis will provide ABC Bank with the tools to design effective retention strategies, tailor customer experiences, and improve overall satisfaction.</p>
<p>This initiative marks a pivotal step for ABC Bank in transforming its approach to customer relationship management. By embracing a data-driven strategy, the bank seeks not only to reduce churn rates but also to foster stronger, more enduring relationships with its customers. The project aligns with the bank’s commitment to excellence in service and innovation in banking solutions, setting a new standard in customer retention and loyalty.</p>
</section>
<section id="data-description" class="level2">
<h2 class="anchored" data-anchor-id="data-description"><strong>Data Description</strong>:</h2>
<p>The dataset used for this project is sourced from Kaggle: <a href="https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset">Bank Customer Churn Dataset</a>. It consists of various customer attributes and a target variable indicating customer churn.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>bank_data <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">'data/Bank Customer Churn Prediction.csv'</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(bank_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>'data.frame':   10000 obs. of  12 variables:
 $ customer_id     : int  15634602 15647311 15619304 15701354 15737888 15574012 15592531 15656148 15792365 15592389 ...
 $ credit_score    : int  619 608 502 699 850 645 822 376 501 684 ...
 $ country         : chr  "France" "Spain" "France" "France" ...
 $ gender          : chr  "Female" "Female" "Female" "Female" ...
 $ age             : int  42 41 42 39 43 44 50 29 44 27 ...
 $ tenure          : int  2 1 8 1 2 8 7 4 4 2 ...
 $ balance         : num  0 83808 159661 0 125511 ...
 $ products_number : int  1 1 3 2 1 2 2 4 2 1 ...
 $ credit_card     : int  1 0 1 0 1 1 1 1 0 1 ...
 $ active_member   : int  1 1 0 0 1 0 1 0 1 1 ...
 $ estimated_salary: num  101349 112543 113932 93827 79084 ...
 $ churn           : int  1 0 1 0 0 1 0 1 0 0 ...</code></pre>
</div>
</div>
<p><strong>Customer_id</strong>: Unique Identifier for a customer.(Primary key)</p>
<p><strong>credit_score</strong>: A score based on their credit card utilization, it is a continuous variable ranging between 0 - 850.</p>
<p><strong>Country</strong>: The country in which the customer belongs to, it is a categorical variable having 3 values, Germany, France and Spain.</p>
<p><strong>Gender</strong>: Gender of the customer, it is categorical having 2 values Male/Female.</p>
<p><strong>Age</strong>: The age of the customer, real valued, greater than 0.</p>
<p><strong>Tenure:</strong> Number of years the customer has been with the bank, Discrete variable.</p>
<p><strong>Balance:</strong> The amount present in customers bank account, continuous varaible.</p>
<p><strong>products_number:</strong> The number of products the customer has with the bank, eg: Credit Card, debit card. It is a discrete variable.</p>
<p><strong>credit_card:</strong> It is a binary variable indicating whether the customer has a credit card or not.</p>
<p><strong>active_member:</strong> It is a binary variable indicating whether the customer has been actively using the banking servies provided by the bank.</p>
<p><strong>estimated_salary</strong>: The estimated salary of the customer, it is a continuous variable.</p>
<p><strong>churn:</strong> It is the dependent variable, 1 indicating if the customer leaves the bank, 0 if not.</p>
</section>
<section id="goal" class="level2">
<h2 class="anchored" data-anchor-id="goal"><strong>Goal</strong></h2>
<p>The goal of the project is to develop a predictive model for ABC Bank that accurately identifies customers at risk of churning, with a strong emphasis on optimizing recall and accuracy. The model aims to minimize false negatives (recall) to capture the maximum number of potential churners, while maintaining high overall accuracy to ensure targeted and effective customer retention strategies.</p>
<p>This approach will enable ABC Bank to proactively address and retain at-risk customers, thereby improving retention rates and gaining insights into customer loyalty and satisfaction factors. The focus on both recall and accuracy ensures a balanced and efficient approach to customer retention efforts.</p>
<section id="exploratory-data-analysis" class="level3">
<h3 class="anchored" data-anchor-id="exploratory-data-analysis"><strong>Exploratory data analysis</strong></h3>
<p>For the ABC Bank customer churn prediction project, a comprehensive exploratory data analysis (EDA) is conducted to understand the characteristics and distribution of the data. This phase is crucial to ensure the quality and reliability of the data before feeding it into the predictive model.</p>
<ol type="1">
<li><p><strong>Dimensionality Check:</strong></p>
<p>The dimensions of the dataset are examined to understand the scale and scope of the data. This includes the number of records and features, which is essential for assessing the computational load and the complexity of the modeling process.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dim</span>(bank_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 10000    12</code></pre>
</div>
</div>
<p>The data has 10K records with 11 independent features and 1 response variable.</p></li>
<li><p><strong>Data Cleaning and Preprocessing:</strong></p>
<p><strong>Dropping Irrelevant Features:</strong> The ‘customer_id’ column is dropped from the dataset. As IDs are unique to each customer and do not contribute to the predictive power of the model, their removal is necessary to streamline the dataset.</p>
<p><strong>Null Value Analysis:</strong> We check for null values across the dataset to identify any gaps in the data. The presence of null values can significantly impact the model’s performance and must be addressed appropriately.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>null_count <span class="ot">&lt;-</span> <span class="fu">colSums</span>(<span class="fu">is.na</span>(bank_data))</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(null_count)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                 null_count
customer_id               0
credit_score              0
country                   0
gender                    0
age                       0
tenure                    0
balance                   0
products_number           0
credit_card               0
active_member             0
estimated_salary          0
churn                     0</code></pre>
</div>
</div>
<p>We can see that the data is pretty clean with no null values in any of the columns.</p></li>
<li><p><strong>Distribution Analysis:</strong></p>
<p><strong>Continuous Variables:</strong> For variables like age, balance, Credit Score etc., we use histograms and boxplots to visualize their distribution. This helps in identifying any skewness, outliers, or unusual patterns in the data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(bank_data<span class="sc">$</span>balance, <span class="at">main =</span> <span class="st">"Histogram of Balance"</span>, <span class="at">xlab =</span> <span class="st">"Balance"</span>, <span class="at">ylab =</span> <span class="st">"Frequency"</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(bank_data<span class="sc">$</span>credit_score, <span class="at">main =</span> <span class="st">"Histogram of Credit Score"</span>, <span class="at">xlab =</span> <span class="st">"Credit Score"</span>, <span class="at">ylab =</span> <span class="st">"Frequency"</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(bank_data<span class="sc">$</span>estimated_salary, <span class="at">main =</span> <span class="st">"Histogram of Estimated Salary"</span>, <span class="at">xlab =</span> <span class="st">"Estimated Salary"</span>, <span class="at">ylab =</span> <span class="st">"Frequency"</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(bank_data<span class="sc">$</span>age, <span class="at">main =</span> <span class="st">"Histogram of Age"</span>, <span class="at">xlab =</span> <span class="st">"Age"</span>, <span class="at">ylab =</span> <span class="st">"Frequency"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Customer_Churn_Prediction_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The histograms for balance and age suggest right-skewed distributions, as most of the data is concentrated on the left side, with the tail extending to the right. For the credit score, the distribution appears to be more symmetrical around the median, suggesting a normal-like distribution. The estimated salary histogram shows a uniform distribution with roughly equal frequency across the salary range.</p>
<p><strong>Categorical Variables:</strong> Variables including country of residence and gender are examined utilizing frequency tables and bar plots to assess their distribution. This approach assists in discerning the spread and prevalence of distinct categories, such as the proportion of customers from Germany, France, and Spain, or the ratio of male to female customers.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Frequency tables</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>country_frequency <span class="ot">&lt;-</span> <span class="fu">table</span>(bank_data<span class="sc">$</span>country)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>gender_frequency <span class="ot">&lt;-</span> <span class="fu">table</span>(bank_data<span class="sc">$</span>gender)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Bar plots</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># For Country</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(bank_data, <span class="fu">aes</span>(<span class="at">x=</span>country)) <span class="sc">+</span> </span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">fill=</span><span class="st">"steelblue"</span>) <span class="sc">+</span> </span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title=</span><span class="st">"Bar Plot of Country"</span>, <span class="at">x=</span><span class="st">"Country"</span>, <span class="at">y=</span><span class="st">"Count"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Customer_Churn_Prediction_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For Gender</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(bank_data, <span class="fu">aes</span>(<span class="at">x=</span>gender)) <span class="sc">+</span> </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">fill=</span><span class="st">"pink"</span>) <span class="sc">+</span> </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title=</span><span class="st">"Bar Plot of Gender"</span>, <span class="at">x=</span><span class="st">"Gender"</span>, <span class="at">y=</span><span class="st">"Count"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="Customer_Churn_Prediction_files/figure-html/unnamed-chunk-5-2.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>We infer that the bank’s customer base has a higher number of male customers than female customers. Additionally, France constitutes the largest customer segment among the three countries analyzed, with Germany and Spain having a similar but smaller number of customer.</p>
<p>These insights are valuable for strategic business decisions. For example, marketing campaigns could be tailored according to the predominant gender, or customer retention strategies could be adjusted to address the differences in customer distribution across countries</p></li>
<li><p><strong>Response Variable Proportion Check:</strong></p>
<p>The proportions of the response variable (churned vs.&nbsp;non-churned customers) are checked. An imbalanced dataset might require special techniques like oversampling, undersampling, or advanced algorithms to ensure the model does not become biased towards the majority class.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(bank_data<span class="sc">$</span>churn)<span class="sc">/</span><span class="fu">nrow</span>(bank_data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
     0      1 
0.7963 0.2037 </code></pre>
</div>
</div>
<p>We can see that 20% of the data points turns out to be churns, 80% are retained customers.</p></li>
</ol>
</section>
</section>
<section id="pre-processing-and-train-test-split" class="level2">
<h2 class="anchored" data-anchor-id="pre-processing-and-train-test-split"><strong>Pre-Processing and Train-Test Split</strong></h2>
<p>First we converted all the categorical variables into factor variables.</p>
<p>Then, we split the data into training and testing data sets in the ration 80:20 to evaluate the model performance on unseen data.</p>
<p>Lets check whether the proportions of the two levels of the response churn are approximately the same in the train, test, and the entire data.</p>
<table class="table">
<caption>Response variable proportion after split.</caption>
<thead>
<tr class="header">
<th>Response</th>
<th>Entire Data</th>
<th>Train</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Churn(1)</td>
<td>79.63</td>
<td>79.63</td>
<td>79.61</td>
</tr>
<tr class="even">
<td>Retain(0)</td>
<td>20.37</td>
<td>20.37</td>
<td>20.39</td>
</tr>
</tbody>
</table>
<p>We can see that the proportions of the two levels of the response variables are approximately same in the train, test and entire data set.</p>
</section>
<section id="evaluation-metrics" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-metrics"><strong>Evaluation Metrics</strong></h2>
<p>The performance of the classification model can be measured by accuracy, precision, recall, F1-Score, and AUC. To evaluate the performance, a confusion matrix model could be used to compute the efficiency of a classification model on group test data with known true values.</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>Predicted-Churn</th>
<th>Predicted-Non-Churn</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Actual-Churn</strong></td>
<td><strong>TP</strong></td>
<td><strong>FN</strong></td>
</tr>
<tr class="even">
<td><strong>Actual-Non-Chirn</strong></td>
<td><strong>FP</strong></td>
<td><strong>TN</strong></td>
</tr>
</tbody>
</table>
<p>The accuracy is the percentage of correct prediction.</p>
<p><span class="math display">\[
Accuracy = \frac{TP+TN}{TP+FP+TN+FN}
\]</span></p>
<p>Precision measures the number of accurately identified churns and overall positive predictions.</p>
<p><span class="math display">\[
Precision=\frac{TP}{TP+FP}
\]</span></p>
<p>Recall is correctly predicted true churners as a proportion of total churners</p>
<p><span class="math display">\[
Recall=\frac{TP}{TP+FN}
\]</span></p>
<p>F1-Score is a composite measure of recall and precision</p>
<p><span class="math display">\[
F1-Score=\frac{2*Precision*Recall}{Precision+Recall}
\]</span></p>
</section>
<section id="application-of-machine-learning-algorithms" class="level1">
<h1><strong>Application of Machine Learning Algorithms</strong></h1>
<section id="binary-logit-modelling" class="level2">
<h2 class="anchored" data-anchor-id="binary-logit-modelling">Binary logit modelling</h2>
<p>The binary logit (or, logistic regression) model is a generalized linear model (GLIM) for explaining binary responses.</p>
<p>Random Component:</p>
<p><span id="eq-logitglim1"><span class="math display">\[
Y_i | \pi_i \sim \mbox{Bernoulli}(\pi_i).  
\tag{1}\]</span></span></p>
<p>Systematic component:</p>
<p><span class="math display">\[
\eta_i =  \beta_0 + \sum_{j=1}^p  \beta_j X_{i,j} = \mathbf{x}_i' \boldsymbol{\beta}.
\]</span> with <span class="math inline">\(\boldsymbol{\beta} = (\beta_0, \beta_1, \ldots,\beta_p)'\)</span>, and <span class="math inline">\(\mathbf{x}_i = (1, X_{i,1},\ldots, X_{i,p})'\)</span>.</p>
<p>The Logit-link Function relates the <span class="math inline">\(i\)</span>th mean response <span class="math inline">\(\pi_i\)</span> to the systematic component:</p>
<p><span id="eq-logitglim2"><span class="math display">\[
\mbox{logit}(\pi_i | \mathbf{x}_i) = \log\left(\frac{\pi_i}{1-\pi_i}\right) = \eta_i.
\tag{2}\]</span></span></p>
<section id="null-logit-model" class="level4">
<h4 class="anchored" data-anchor-id="null-logit-model">Null Logit Model :</h4>
<p>In the initial stage of our analytical process, we constructed a null logistic regression model using only an intercept, without any explanatory variables.</p>
<p>The null model’s residual deviance stands at 8085.7 with an Akaike Information Criterion (AIC) of 8087.7.</p>
</section>
<section id="full-logit-model" class="level4">
<h4 class="anchored" data-anchor-id="full-logit-model">Full Logit Model</h4>
<p>We advanced our analysis by developing a comprehensive logistic regression model that included a full set of predictor variables. This enhanced model demonstrated a notable improvement over the baseline model, reflected by a lower residual deviance of 6897.4 and an Akaike Information Criterion (AIC) of 6921.4.</p>
</section>
<section id="performance" class="level4">
<h4 class="anchored" data-anchor-id="performance">Performance:</h4>
<p>The performance of the complete model was assessed on the test with the help of a confusion matrix, which is a tabular representation that delineates the efficacy of a classification algorithm.</p>
<table class="table">
<caption>Using the confusion matrix, we were able to determine the full model’s accuracy, which represents the ratio of correctly predicted instances to the overall number of predictions. The model displayed an accuracy rate of approximately 81.95%, indicating that it was able to correctly predict customer churn in roughly 82 out of every 100 cases.</caption>
<thead>
<tr class="header">
<th></th>
<th><strong>Predicted No</strong></th>
<th><strong>Predicted Yes</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Actual No</strong></td>
<td>1547</td>
<td>46</td>
</tr>
<tr class="even">
<td><strong>Actual Yes</strong></td>
<td>315</td>
<td>93</td>
</tr>
</tbody>
</table>
</section>
<section id="stepwise-variable-selection" class="level4">
<h4 class="anchored" data-anchor-id="stepwise-variable-selection">Stepwise Variable Selection</h4>
<p>After applying varaible selection from both sides, we ended up removing 3 predictors from the model namely estimated_salary, credit_card and tenure.</p>
<p>To enhance our model’s efficiency, we applied a stepwise selection process to distill the predictor variables, focusing on their statistical relevance and their overall impact on the model’s performance. This refined approach resulted in a more streamlined model with a residual deviance of 6900 and an Akaike Information Criterion (AIC) of 6918.7.</p>
<p>The AIC is 6918.7 which is slightly different(3units less) from the full model, but the residual deviance has slightly increased(by 3 units).</p>
<p>But, if we look at the BIC of both the models, the model with variable selection has lower BIC compared to full model, again its slightly different(20 units less).</p>
</section>
<section id="reduced-logit-model-performance" class="level4">
<h4 class="anchored" data-anchor-id="reduced-logit-model-performance">Reduced Logit Model Performance</h4>
<table class="table">
<caption>The accuracy of the reduced logistic model is approximately 82.01%, which means that the model correctly predicted whether a customer would churn in about 82 out of every 100 cases.</caption>
<thead>
<tr class="header">
<th></th>
<th><strong>Predicted No</strong></th>
<th><strong>Predicted Yes</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Actual No</strong></td>
<td>1547</td>
<td>46</td>
</tr>
<tr class="even">
<td><strong>Actual Yes</strong></td>
<td>314</td>
<td>94</td>
</tr>
</tbody>
</table>
<p>The accuracy has slightly increased compared to the full model, but this is a negligible change.</p>
<p>The stepwise model shows a slightly higher number of true positives and a slightly lower number of false negatives, which can be seen as a marginal improvement in predictive performance.</p>
<p><strong>Probit Regression for Binary Responses</strong></p>
<p>The probit link function is an alternative link function.</p>
<p>The probit model slightly has higher AIC and higher residual deviance comapred to the reduced logit model by 2 units in both the metrics.</p>
<p>As the difference is negligible, lets check the <strong>probit models performance</strong> on the <strong>test data</strong>.</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Predicted No</strong></th>
<th><strong>Predicted Yes</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Actual No</strong></td>
<td>1552</td>
<td>41</td>
</tr>
<tr class="even">
<td><strong>Actual Yes</strong></td>
<td>323</td>
<td>85</td>
</tr>
</tbody>
</table>
<p><strong>Accuracy (Approximately 81.81%)</strong>: This tells us that overall, the model correctly predicts whether a customer will churn or not about 82% of the time.</p>
<p><strong>Precision (Approximately 67.46%)</strong>: When the model predicts that a customer is going to churn, it is correct about two-thirds of the time.</p>
<p><strong>Recall (Approximately 20.83%)</strong>: The model only correctly identifies about 21% of the customers who will actually churn. This low recall rate means that many customers who are likely to churn are not being flagged by the model, which could lead to a significant loss of customers if they are not addressed.</p>
<p><strong>F1 Score (Approximately 0.318)</strong>: This low score indicates that the model has a poor balance between precision and recall. It is not very effective at identifying customers who will churn (low recall), even though when it does predict churn, it is reasonably precise.</p>
</section>
</section>
<section id="decision-trees" class="level2">
<h2 class="anchored" data-anchor-id="decision-trees"><strong>Decision Trees:</strong></h2>
<p>A&nbsp;<strong>binary tree</strong>&nbsp;is constructed by repeatedly by splitting a node into two child nodes. The split is made based on the predictor (feature) which results in the largest possible reduction in heterogeneity of the binary response variable.</p>
<section id="impurity-measure" class="level4">
<h4 class="anchored" data-anchor-id="impurity-measure">Impurity measure:</h4>
<p><span class="math display">\[
\mbox{Gini index} = 1 - \sum_{j=1}^J p^2_j,  
\]</span></p>
<p>The initial misclassification rate of the decision tree, represented by the root node error, was approximately 20.365%, which is the proportion of incorrect predictions before the tree made any splits. To control the tree’s growth and mitigate the risk of overfitting, the complexity parameter (CP) was set to a threshold of 0.001.</p>
<p><strong>Decision Tree Model Performance</strong></p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Predicted No</strong></th>
<th><strong>Predicted Yes</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Actual No</strong></td>
<td>1517</td>
<td>76</td>
</tr>
<tr class="even">
<td><strong>Actual Yes</strong></td>
<td>185</td>
<td>223</td>
</tr>
</tbody>
</table>
<p>The <strong>accuracy</strong> of the model, which indicates the overall correct predictions, is 86.957%. This means that the model correctly predicts the churn status for approximately 87 out of 100 customers.</p>
<p>The <strong>precision</strong> for class 1, or the ‘predicted yes’ class, is 0.7458, indicating that when the model predicts a customer will churn, it is correct about 74.58% of the time.</p>
<p>The <strong>recall</strong> for the actual class 1 is 0.5466, suggesting that the model correctly identifies 54.66% of the customers who actually churn.</p>
<p>The <strong>F1 score</strong>, which balances precision and recall, is 0.6308, providing a single measure of the model’s accuracy when the class distribution is imbalanced.</p>
</section>
</section>
<section id="prunning-the-decision-tree" class="level2">
<h2 class="anchored" data-anchor-id="prunning-the-decision-tree"><strong>Prunning the Decision Tree</strong></h2>
<p>Pruning a decision tree means simplifying it by removing unnecessary parts. This prevents it from being too complex and helps it work better with new data. We can cut off less important branches or set rules for when to stop splitting.</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Predicted No</strong></th>
<th><strong>Predicted Yes</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Actual No</strong></td>
<td>1547</td>
<td>46</td>
</tr>
<tr class="even">
<td><strong>Actual Yes</strong></td>
<td>206</td>
<td>202</td>
</tr>
</tbody>
</table>
<p>The <strong>Accuracy</strong> has slightly increased from 86.9 in full tree to 87.4 in pruned tree.</p>
<p>The <strong>Precision</strong> significantly increased from 74% in full tree to 81% in pruned tree.</p>
<p>But, the <strong>Recall</strong> has reduced from 54% in full tree to 49% in pruned tree.</p>
<p>Recall is also been reduced from 63% to 61.4%.</p>
<p>The pruned model seems to perform better in terms of accuracy and precision, which could make it preferable if the prediction task values the correctness of positive predictions over the ability to identify all positive instances.</p>
<p>The full decision tree model, despite being less precise, is better at capturing more of the positive instances (higher recall, it would be the preferred choice as the primary objective is to capture as many true churn cases as possible.</p>
</section>
<section id="randomforest-classifier" class="level2">
<h2 class="anchored" data-anchor-id="randomforest-classifier"><strong>RandomForest classifier</strong></h2>
<p>We utilized the ranger package in R to construct a Random Forest model. This approach is renowned for its high accuracy and resilience, making it a valuable tool for assessing the likelihood of customers leaving a service or product. The process involves training the model by generating multiple decision trees and then using their collective insight to make predictions regarding customer churn.</p>
<p><strong>Model Parameters:</strong></p>
<p><strong>Number of Trees:</strong> The model has been trained with 500 trees, which means it aggregates the decisions of 500 individual decision trees.</p>
<p><strong>Sample Size:</strong> It used 7999 samples for training, providing a substantial dataset for the trees to learn from.</p>
<p><strong>Number of Independent Variables (Mtry):</strong> At each split in the decision trees, 3 variables were considered. This is a form of random subspace method that helps in reducing variance and improving model performance.</p>
<p><strong>Minimum Node Size:</strong> The model allows splitting down to a single sample in a node, aiming for very detailed and fine-grained predictions.</p>
<p><strong>Variable Importance Mode:</strong> It uses ‘impurity’, likely referring to how much each variable decreases the impurity in the model, with the Gini index as a measure.</p>
<section id="evaluation-of-the-model" class="level3">
<h3 class="anchored" data-anchor-id="evaluation-of-the-model">Evaluation of the model<strong>:</strong></h3>
<p><strong>OOB Prediction Error:</strong> The Out-Of-Bag error, which is an estimate of prediction error, is 14.34%. This means that when the model predicts on new data, it is expected to be about 85.66% accurate, based on the samples not used in the training (bootstrap sample) of each tree.</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Predicted No</strong></th>
<th><strong>Predicted Yes</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Actual No</strong></td>
<td>1545</td>
<td>48</td>
</tr>
<tr class="even">
<td><strong>Actual Yes</strong></td>
<td>191</td>
<td>217</td>
</tr>
</tbody>
</table>
<p><strong>Accuracy</strong> isapproximately 88.06%, indicating that the model correctly predicted the outcome for 88.06% of the cases in the dataset.</p>
<p><strong>Precision</strong> is approximately 81.89%, which means that when the model predicted a customer as churned, it was correct about 81.89% of the time.</p>
<p><strong>Recall</strong> is a pproximately 53.19%, indicating that the model identified 53.19% of all actual churned cases correctly.</p>
<p><strong>F1 Score is</strong> 64.5%, this metric balances the precision and recall and is particularly useful when the class distribution is imbalanced.</p>
</section>
</section>
<section id="xtreme-gradient-boosting" class="level2">
<h2 class="anchored" data-anchor-id="xtreme-gradient-boosting"><strong>Xtreme Gradient Boosting</strong></h2>
<p>The XGBoost algorithm, known for its efficiency and effectiveness in handling classification and regression tasks, was utilized. Its performance was evaluated using the Area Under the Receiver Operating Characteristic Curve (AUC-ROC) metric across multiple iterations for both training and test sets. The AUC-ROC metric assesses a model’s ability to differentiate between the classes.</p>
<p>Key observations from the training process include:</p>
<p>The AUC on the <strong>training data</strong> began at 0.736432 and displayed a consistent upward trend, culminating at 0.867467 by the 10th iteration. This indicates progressive learning and improvement in the model’s ability to correctly classify the training data.</p>
<p>For the <strong>test data</strong>, the AUC started at 0.764130 and experienced variations throughout the iterations. It saw a high of 0.871254 by the 10th iteration, showing a general increasing trend but with some fluctuations across the iterations.</p>
<p>The trend of the training AUC steadily increasing suggests the model is continuously learning from the training data. However, the test AUC’s variability, particularly around values of 0.79 to 0.87, implies a risk of overfitting if training rounds continue to increase without corresponding improvements on the test set.</p>
<p><strong>XGB Model Performance</strong></p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><strong>Predicted No</strong></th>
<th><strong>Predicted Yes</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Actual No</strong></td>
<td>1521</td>
<td>72</td>
</tr>
<tr class="even">
<td><strong>Actual Yes</strong></td>
<td>202</td>
<td>206</td>
</tr>
</tbody>
</table>
<p><strong>Accuracy (86.31%)</strong>: This indicates that the model correctly identified whether a customer would churn or not about 86% of the time. It’s a general measure of the model’s overall performance but doesn’t specifically indicate how well the model identifies churned customers versus those who stay.</p>
<p><strong>Precision (74.10%)</strong>: Precision reflects the model’s ability to correctly identify true churn cases out of all cases it predicted as churn. In practical terms, when the model predicts a customer will churn, there’s a 74.10% chance it is correct.This is important for a business because it can help prioritize which customers to target with retention strategies.</p>
<p><strong>Recall (50.49%)</strong>: Recall measures the model’s ability to find all the actual churn cases. A recall of about 50.49% means the model correctly identifies half of the customers who will churn. For a business, this could indicate that the model is missing half of the customers at risk of churning, which could be costly if these customers are not engaged with retention efforts.</p>
<p>An <strong>F1 score</strong> of <strong>0.600</strong> suggests the model is moderately effective but not exceptional at predicting customer churn. It indicates there is room for improvement, especially in terms of recall, to ensure that more customers who are likely to churn are accurately identified.</p>
<p>For a business, these metrics would suggest that while the model is fairly good at predicting who will churn, it tends to miss a significant number of customers who do churn (as indicated by the recall).</p>
<p>This could be problematic if the cost of missing a customer who churns is high.</p>
</section>
<section id="results-from-the-analyses" class="level2">
<h2 class="anchored" data-anchor-id="results-from-the-analyses"><strong>Results from the Analyses:</strong></h2>
<p>In this comprehensive analysis, we delve into the performance evaluation of four key machine learning models: Logistic Regression, Decision Trees, Gradient Boosting, and Random Forest. We gauge the effectiveness of these models in predicting customer churn by assessing critical metrics such as accuracy, precision, recall, and F1-score.</p>
<section id="logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression">Logistic Regression</h3>
<p>Performance Metrics:</p>
<ul>
<li><p>Accuracy: 81.95%</p></li>
<li><p>Precision: 66.90%</p></li>
<li><p>Recall: 22.79%</p></li>
<li><p>F1-Score: 34.00%</p></li>
</ul>
<p>Analysis: Logistic Regression served as a reasonable baseline. However, its limitation in handling non-linear relationships and interactions among variables led to lower recall rates.</p>
</section>
<section id="decision-trees-1" class="level3">
<h3 class="anchored" data-anchor-id="decision-trees-1">Decision Trees</h3>
<p>Performance Metrics:</p>
<ul>
<li><p>Accuracy: 86.95%</p></li>
<li><p>Precision: 74.58%</p></li>
<li><p>Recall: 54.66%</p></li>
<li><p>F1-Score: 63.08%</p></li>
</ul>
<p>Analysis: Decision Trees exhibited improved recall, indicating better identification of actual churn cases. Nevertheless, the model’s tendency to overfit resulted in lower precision.</p>
</section>
<section id="gradient-boosting" class="level3">
<h3 class="anchored" data-anchor-id="gradient-boosting">Gradient Boosting</h3>
<p>Performance Metrics:</p>
<ul>
<li><p>Accuracy: 86.30%</p></li>
<li><p>Precision: 74.10%</p></li>
<li><p>Recall: 50.49%</p></li>
<li><p>F1-Score: 60.05%</p></li>
</ul>
<p>Analysis: Gradient Boosting showed enhanced precision but slightly lower recall compared to Decision Trees. The model performed well but demanded more computational resources and tuning.</p>
</section>
<section id="random-forest" class="level3">
<h3 class="anchored" data-anchor-id="random-forest">Random Forest</h3>
<p>Performance Metrics:</p>
<ul>
<li><p>Accuracy: 88.06%</p></li>
<li><p>Precision: 81.89%</p></li>
<li><p>Recall: 53.19%</p></li>
<li><p>F1-Score: 64.49%</p></li>
</ul>
<p>Analysis: The Random Forest model excelled, striking the best balance between precision and recall. Its ensemble approach effectively handled the dataset’s intricacies and noise, resulting in higher overall accuracy and more reliable churn predictions.</p>
</section>
<section id="random-forest-the-preferred-model" class="level3">
<h3 class="anchored" data-anchor-id="random-forest-the-preferred-model"><strong>Random Forest: The Preferred Model</strong></h3>
<p>After thorough analysis and comparison, the Random Forest model emerges as the optimal choice for several compelling reasons:</p>
<p>Accuracy and Precision: The Random Forest model achieved an impressive accuracy rate of 88.06%, demonstrating its proficiency in correctly classifying both churn and non-churn cases. Furthermore, it boasted a precision rate of 81.89%, signifying that a substantial proportion of the model’s churn predictions were accurate.</p>
<p>Precision-Recall Balance: While excelling in precision, the model also maintained a commendable recall rate of 53.19%. This equilibrium is pivotal in churn prediction, ensuring a high identification of actual churn cases while minimizing false positives.</p>
<p>Robustness and Stability: Random Forest’s ensemble nature, which aggregates results from multiple decision trees, mitigates overfitting risks often encountered in complex models. This robustness makes it an ideal choice for handling the diverse and intricate nature of customer data.</p>
<p>Insightful Feature Importance: A noteworthy advantage of the Random Forest model lies in its capability to rank feature importance. In our case, variables such as age, account balance, and credit score emerged as influential factors, offering valuable insights for strategic decision-making in customer retention initiatives.</p>
</section>
<section id="comparative-performance" class="level3">
<h3 class="anchored" data-anchor-id="comparative-performance"><strong>Comparative Performance</strong></h3>
<p>Although Logistic Regression and Gradient Boosting models exhibited commendable performance, they fell short in either precision or recall when compared to the Random Forest model. The Decision Tree model, while offering slightly higher recall, lacked the precision and stability of Random Forest.</p>
<p>In conclusion, our analysis strongly advocates for the adoption of the Random Forest model for customer churn prediction. It not only delivers high accuracy and a balanced precision-recall trade-off but also provides indispensable insights into the factors influencing customer decisions to leave the bank. These insights can serve as a guiding light for ABC Bank’s strategic endeavors aimed at elevating customer retention and satisfaction.</p>
</section>
</section>
<section id="summary-and-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="summary-and-conclusion"><strong>Summary and Conclusion:</strong></h2>
<p>In our project, “Customer Churn Prediction” we harnessed the power of machine learning algorithms to forecast customer churn. Our analysis was conducted using the Bank Customer Churn Dataset sourced from Kaggle, which encompassed a variety of customer attributes. After meticulously preprocessing the data, we applied several models, including Logistic Regression, Decision Trees, Gradient Boosting, and Random Forest, for our predictions. Our assessment criteria included accuracy, precision, recall, and F1-score.</p>
<p>The Random Forest model emerged as the standout performer, achieving an accuracy of 88.06%, a precision rate of 81.89%, a recall rate of 53.19%, and an F1-score of 64.49%. Notably, this model not only delivered the highest accuracy but also exhibited a robust balance between precision and recall. It effectively managed the intricacies and noise within the data, rendering it the most dependable choice for our churn prediction objectives.</p>
<p><strong>Insights Gained</strong></p>
<p>Our analysis uncovered several pivotal insights:</p>
<p>Influential Predictors: Factors such as age, account balance, and credit score emerged as influential predictors of churn. Grasping the significance of these variables can guide the formulation of targeted customer retention strategies.</p>
<p>Model Robustness: The ensemble approach adopted by the Random Forest model, which aggregates multiple decision trees, effectively mitigated the risk of overfitting—a common challenge in predictive modeling.</p>
<p>Strategic Recommendations: Leveraging the insights from our model, ABC Bank can craft data-driven strategies aimed at enhancing customer engagement and curtailing churn rates.</p>
<p><strong>Conclusions</strong></p>
<p>The superior performance of the Random Forest model in our study underscores the transformative potential of machine learning in reshaping customer retention strategies within the banking industry. By accurately pinpointing high-risk customers and shedding light on churn-inducing factors, our model equips ABC Bank with the tools to take proactive steps in bolstering customer satisfaction and loyalty.</p>
<p>Furthermore, our project underscores the pivotal role of a data-driven approach in shaping decision-making processes. The predictive model not only serves as a churn prediction tool but also serves as a catalyst for instilling a culture of data-driven strategy development within the organization.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>